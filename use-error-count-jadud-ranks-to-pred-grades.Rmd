---
title: "SRL-use total error counts to predict learning outcomes"
author: "Joyce Zhang"
date: '2023-03-29'
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)

library(dplyr)
library(reshape2)
library(stats)
library(moments)
library(tidyr)
library(kableExtra)
library(data.table)
library(ggplot2)
library(Rfit)

Sys.setlocale( 'LC_ALL','C' ) 
```
# 1. import data
## 1.1 import data from "hwXX-snapshot-summary.csv" for HW0-8 (except for HW1, HW2)
create dataframe df. Each row shows the following 4 measures for each snapahot (see example below)
- c_error: 1: snapshot with a compiler error, 0: snapshot compiled
- r_error: the number of runtime errors in a snapshot
- t_failed: the number of tests that failed (includes tests that failed because of the runtime error)
- t_success: the number of tests that ended with success

```{r import data, warning=FALSE}
df <- 
  rbind(read.csv("data-snapshot-summary/hw00-snapshot-summary.csv") %>% mutate(HW = "HW0"),
        read.csv("data-snapshot-summary/hw03-snapshot-summary.csv") %>% mutate(HW = "HW3"),
        read.csv("data-snapshot-summary/hw04-snapshot-summary.csv") %>% mutate(HW = "HW4"),
        read.csv("data-snapshot-summary/hw05-snapshot-summary.csv") %>% mutate(HW = "HW5"),
        read.csv("data-snapshot-summary/hw06-snapshot-summary.csv") %>% mutate(HW = "HW6"),
        read.csv("data-snapshot-summary/hw07-snapshot-summary.csv") %>% mutate(HW = "HW7"),
        read.csv("data-snapshot-summary/hw08-snapshot-summary.csv") %>% mutate(HW = "HW8"))

colnames(df)[1] <- "colName"

df <- 
  separate(
    df,
    colName,
    c("student","snapshot","c_error","r_error","t_failed","t_success"),
    sep = ";")

df[3:6] <- apply(df[3:6],2,as.character)
df[3:6] <- apply(df[3:6],2,as.numeric)

kable(df[c(1:5),])

```


**agggregate df to student level**

create dataframe 'df_student'. each row shows for each student and HW, the total number of snapshots captured (N_snaphot), total number of snapshots containing compiler errors (C_totalSnap), its percentage (C_perc), total number of snapshot containing runtime errors (R_totalSnap) and its percentages (R_perc), total number of runtime errors (a snapshot can have more than 1 runtime error)(R_totalError), and the average number of runtime errors (R_avgError). See the top 5 rows of df_student dataframe below  

```{r error measures, warning=FALSE}
df_student <- 
  df %>% 
  group_by(student,HW) %>% 
  summarise(
    N_snapshot = n(),
    C_totalSnap = sum(c_error),
    C_perc = C_totalSnap/N_snapshot,
    R_totalSnap = sum(r_error>0),
    R_perc = sum(r_error>0)/N_snapshot,
    R_totalError = sum(r_error),
    R_avgError = mean(r_error)
  )

kable(df_student[1:5,])

```

## 1.2 import data from "hwXX-compiler-errors.csv"files for the same HW
similarly, we imported the compiler errors from the "hwXX-compiler-errors.csv" files. aggregated them to the student level, creating a dataframe (df_c_student). This dataframe gives the total number of compiler errors a student has made for each HW. see table below

```{r number of compiler errors, warning=FALSE}

df_c <- 
  rbind(
    read.csv("data-errors/hw00-compiler-errors.csv") %>% mutate(HW = "HW0"),
    read.csv("data-errors/hw03-compiler-errors.csv") %>% mutate(HW = "HW3"),
    read.csv("data-errors/hw04-compiler-errors.csv") %>% mutate(HW = "HW4"),
    read.csv("data-errors/hw05-compiler-errors.csv") %>% mutate(HW = "HW5"),
    read.csv("data-errors/hw06-compiler-errors.csv") %>% mutate(HW = "HW6"),
    read.csv("data-errors/hw07-compiler-errors.csv") %>% mutate(HW = "HW7"),
    read.csv("data-errors/hw08-compiler-errors.csv") %>% mutate(HW = "HW8"))

colnames(df_c)[1] <- "colName"

df_c <- 
  separate(
    df_c,
    colName,
    c("student","snapshot","file","line","error"),
    sep = ";")

df_c_student <- 
  df_c %>% 
  group_by(HW,student) %>% 
  summarise(C_totalError = n())

kable(df_c_student[1:5,])
```

**combine df_student with df_c_student**
see the top 5 rows below.
```{r combine runtime and compiler errors, warning=FALSE}
df_error <- left_join(df_student, df_c_student, by = c("student","HW"))


kable(df_error[1:5,])

```

\newpage

# 2 descriptive stats for error rate by HW
```{r descriptive stats by HW}
df_summary_byHW <- 
  df_error %>% 
  group_by(HW) %>% 
  summarise(
    N_student = n(),
    avg_N_snapshot = round(mean(N_snapshot, na.rm = T),1),
    avg_N_runtime = round(mean(R_totalError,na.rm = T),1),
    avg_N_compiler = round(mean(C_totalError,na.rm = T),1)
  )

kable(df_summary_byHW)
```

\newpage
# 3 use total number of runtime/compiler errors to predict HW performance
the following regression is used to estimate the relationship between the total number of errors and HW performance (rank). The results show that controlling for HW number, total number of runtime and compiler errors are negatively associated with the ranking in HW performance. number of compiler errors is more significantly as well as has higher impact than the numner of runtime errors.

**lm_HWgrade <- summary(lm(rank~HW+R_totalError+C_totalError, data = df_grade))**
```{r predicting HW performance}
grade <-
  read.csv("grades/df_HWGrades_grade.csv") %>%
  melt("HashedId") %>%
  group_by(variable) %>%
  mutate(rank = rank(value))

colnames(grade)[3] <- "score"
df_grade <- inner_join(df_error,grade, by = c("student" = "HashedId", "HW"= "variable"))
lm_HWgrade <- summary(lm(rank~HW+R_totalError+C_totalError, data = df_grade))

lm_HWgrade

```

\newpage

# 4 use totalt number of runtime/compiler errors to predict mid-term 1 & 2 grades
## 4.1 the distribution of midterm1 & 2

```{r predicting mid-term grade}
courseGrades <- read.csv("grades/CourseGrades.csv")
colnames(courseGrades)[1] <- "HASH_ID"
courseGrades$Grade = as.numeric(as.character(courseGrades$Grade))

midterm <- 
  inner_join(
    read.csv("grades/Midterm1_Fall_2020.csv"),
    read.csv("grades/Midterm2_Fall_2020.csv"), 
    by = "HASH_ID") %>% 
  rename(Midterm1= Total.Score.x, Midterm2 = Total.Score.y) %>% 
  inner_join(courseGrades,by="HASH_ID") %>% 
  subset(!is.na(Midterm1) & !is.na(Midterm2) & !is.na(Grade))

Error <- 
  df_error %>% 
  select(student, HW, R_totalError, C_totalError) %>% 
  pivot_wider(
    id_cols = student,
    names_from = HW,
    values_from = c("R_totalError", "C_totalError")
  )



# both runtime and compile time errors ----
allError <- left_join(midterm,Error, by = c("HASH_ID" = "student"))
#sum up HW3,4 for midterm 1 and 3-8 for midterm2
allError$totalRuntime_HW04 <- apply(allError[,c(6:7)],1,sum, na.rm = T)
allError$totalRuntime_HW08 <- apply(allError[,c(6:11)],1,sum,na.rm = T)
allError$totalcompile_HW04 <- apply(allError[,c(13:14)],1,sum,na.rm = T)
allError$totalcompile_HW08 <- apply(allError[,c(13:18)],1,sum,na.rm = T)

#average HW3,4 for midterm 1 and 3-8 for midterm2
allError$AvgRuntime_HW04 <- apply(allError[,c(6:7)],1,mean, na.rm = T)
allError$AvgRuntime_HW08 <- apply(allError[,c(6:11)],1,mean,na.rm = T)
allError$Avgcompile_HW04 <- apply(allError[,c(13:14)],1,mean,na.rm = T)
allError$Avgcompile_HW08 <- apply(allError[,c(13:18)],1,mean,na.rm = T)
```

midterm 1 histogram (skewness = -1.24; kurtosis = 4.32)
```{r fig.align = 'center'}
hist(midterm$Midterm1)
```

midterm 2 histogram (skewness = -1.23; kurtosis = 3.83)
```{r fig.align = 'center'}
hist(midterm$Midterm2)
```

Because both grades are not heavily skewed, in the following analyses we did not transform the grades into ranks.

course grades histogram (skewness = -1.58; kurtosis = 5.19)
```{r fig.align = 'center'}
hist(midterm$Grade)
```


## 4.2 predicting mid-term 1 grades
I tried two approaches to predict mid-term 1 grades (individual HW errors vs. aggregated errors for HW0-4)
```{r, echo=TRUE}
# predict midterm1
# linear regression
# model 1 (individual)
summary(lm(Midterm1 ~
             R_totalError_HW3+R_totalError_HW4+
             C_totalError_HW3+C_totalError_HW4,
           data = allError))
#model 2 (average)
summary(lm(Midterm1~AvgRuntime_HW04+Avgcompile_HW04,data = allError))

# model 3 (sum)
summary(lm(Midterm1~totalRuntime_HW04+totalcompile_HW04,data = allError))

# rank based regression
summary(rfit(Midterm1 ~
             R_totalError_HW3+R_totalError_HW4+
             C_totalError_HW3+C_totalError_HW4,
           data = allError))
#model 2 (average)
summary(rfit(Midterm1~AvgRuntime_HW04+Avgcompile_HW04,data = allError))

# model 3 (sum)
summary(rfit(Midterm1~totalRuntime_HW04+totalcompile_HW04,data = allError))


```

## 4.3 predicting mid-term 2 grades
```{r echo=TRUE}
#model 1 (individual)
summary(lm(Midterm2 ~
             R_totalError_HW3+R_totalError_HW4 +
             R_totalError_HW5+R_totalError_HW6+
             R_totalError_HW7+R_totalError_HW8+
             C_totalError_HW3+C_totalError_HW4+
             C_totalError_HW5+C_totalError_HW6+
             C_totalError_HW7++C_totalError_HW8,
           data = allError))

#model 2 (average)
summary(lm(Midterm2~AvgRuntime_HW08 + Avgcompile_HW08, data = allError))

#model 3 (sum)
summary(lm(Midterm2~totalRuntime_HW08 + totalcompile_HW08, data = allError))

#rank based
#model 1 (individual)
summary(rfit(Midterm2 ~
             R_totalError_HW3+R_totalError_HW4 +
             R_totalError_HW5+R_totalError_HW6+
             R_totalError_HW7+R_totalError_HW8+
             C_totalError_HW3+C_totalError_HW4+
             C_totalError_HW5+C_totalError_HW6+
             C_totalError_HW7++C_totalError_HW8,
           data = allError))

#model 2 (average)
summary(rfit(Midterm2~AvgRuntime_HW08 + Avgcompile_HW08, data = allError))

#model 3 (sum)
summary(rfit(Midterm2~totalRuntime_HW08 + totalcompile_HW08, data = allError))


```

## 4.4 predicting course grades
```{r echo=TRUE}
#model 1 (individual)
summary(lm(Grade ~
             R_totalError_HW3+R_totalError_HW4 +
             R_totalError_HW5+R_totalError_HW6+
             R_totalError_HW7+R_totalError_HW8+
             C_totalError_HW3+C_totalError_HW4+
             C_totalError_HW5+C_totalError_HW6+
             C_totalError_HW7++C_totalError_HW8,
           data = allError))

#model 2 (average)
summary(lm(Grade~AvgRuntime_HW08 + Avgcompile_HW08, data = allError))

#model 3 (sum)
summary(lm(Grade~totalRuntime_HW08 + totalcompile_HW08, data = allError))

# rank based
#model 1 (individual)
summary(rfit(Grade ~
             R_totalError_HW3+R_totalError_HW4 +
             R_totalError_HW5+R_totalError_HW6+
             R_totalError_HW7+R_totalError_HW8+
             C_totalError_HW3+C_totalError_HW4+
             C_totalError_HW5+C_totalError_HW6+
             C_totalError_HW7++C_totalError_HW8,
           data = allError))

#model 2 (average)
summary(rfit(Grade~AvgRuntime_HW08 + Avgcompile_HW08, data = allError))

#model 3 (sum)
summary(rfit(Grade~totalRuntime_HW08 + totalcompile_HW08, data = allError))
```

\newpage
# 5 use jadud to predict mid-term 1 & 2 grades
## 5.1 descriptive stats and density plot of jadud error quotient

```{r import jadud csv, warning=FALSE}

jadud <- read.csv("jadud.csv")

jadud_long <- melt(jadud, id = "X...student_id")

ggplot(jadud_long,aes(value,color=variable))+geom_density()+
  labs(title = "jadud density plot for each HW")+
  theme_bw()

```

```{r}
jadud_long %>% 
  group_by(HW = variable) %>% 
  summarise(Avg = round(mean(value,na.rm=T),2), Stdv = round(sd(value, na.rm = T),2)) %>% 
  kable() %>% 
  kable_styling(bootstrap_options = "bordered",full_width = F)
```

\newpage
## 5.2 use jadud EQ to predict midterm 1 grades
```{r}
jadud_df <- left_join(midterm,jadud, by = c("HASH_ID" = "X...student_id"))

jadud_df$Avg_HW04 = apply(jadud_df[c(5,6)],1,mean,na.rm=TRUE)
jadud_df$Avg_HW08 = apply(jadud_df[c(5,10)],1,mean,na.rm=TRUE)

jadud_df$Sum_HW04 = apply(jadud_df[c(5,6)],1,sum,na.rm=TRUE)
jadud_df$Sum_HW08 = apply(jadud_df[c(5,10)],1,sum,na.rm=TRUE)
```

```{r use jadud to predict midterm 1 grade, echo=TRUE}
# model 1 (individual)
summary(lm(Midterm1 ~ jadud_hw_03 + jadud_hw_04, data = jadud_df))

# model 2 (average)
summary(lm(Midterm1 ~ Avg_HW04, data = jadud_df))

# model 3 (sum)
summary(lm(Midterm1 ~ Sum_HW04, data = jadud_df))
```

## 5.2 use jadud EQ to predict midterm 2 grades
```{r use jadud to predict midterm 2 grades}
# model 1 (individual)
summary(lm(Midterm2 ~ jadud_hw_03 + jadud_hw_04 + jadud_hw_05 + jadud_hw_06 + jadud_hw_07 + jadud_hw_08, data = jadud_df))

# model 2 (average)
summary(lm(Midterm2 ~ Avg_HW08, data = jadud_df))

# model 3 (sum)
summary(lm(Midterm2 ~ Sum_HW08, data = jadud_df))
```

## 5.3 use jadud EQ to predict course grades
```{r use jadud to predict course grades}
# model 1 (individual)
summary(lm(Grade ~ jadud_hw_03 + jadud_hw_04 + jadud_hw_05 + jadud_hw_06 + jadud_hw_07 + jadud_hw_08, data = jadud_df))

# model 2 (average)
summary(lm(Grade ~ Avg_HW08, data = jadud_df))

# model 3 (sum)
summary(lm(Grade ~ Sum_HW08, data = jadud_df))
```

\newpage

# 6 use HW performance (ranks) to predict midterm grades

```{r}
grade_wide = select(grade,HashedId,variable,score) %>% spread(key = variable, value = score) #change score to rank or vice versa
grade_wide <- left_join(midterm,grade_wide, by = c("HASH_ID" = "HashedId"))

```
## 6.1 use HW ranks to predict midterm 1
```{r use HW rank to predict midterm 1, echo= TRUE}
summary(lm(Midterm1 ~ HW3 + HW4, data = grade_wide))

summary(rfit(Midterm1 ~ HW3 + HW4, data = grade_wide))
```

## 6.2 use HW ranks to predict midterm 2
```{r}
summary(lm(Midterm2 ~ HW3+HW4+HW5+HW6+HW7+HW8, data = grade_wide))

summary(rfit(Midterm2 ~ HW3+HW4+HW5+HW6+HW7+HW8, data = grade_wide))
```

## 6.3 use HW ranks to predict course grades
```{r}
summary(lm(Grade ~ HW3+HW4+HW5+HW6+HW7+HW8, data = grade_wide))

summary(rfit(Grade ~ HW3+HW4+HW5+HW6+HW7+HW8, data = grade_wide))
```

